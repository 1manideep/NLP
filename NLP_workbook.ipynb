{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Models (without using any libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, text, n=None):\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.split_text = self.process()\n",
    "\n",
    "    def process(self):\n",
    "        text = self.text.lower()\n",
    "        text = text.replace('.', ' ')\n",
    "        text = text.replace(',', ' ')\n",
    "        split_text = text.split()\n",
    "        return split_text\n",
    "\n",
    "    def ngram_context(self):\n",
    "        ngram_list, context_list  = [], []\n",
    "        vocab = set()\n",
    "        n = self.n\n",
    "        for i in range(0, len(self.split_text) - n + 1):\n",
    "            ngram = self.split_text[i:i + n]\n",
    "            context = self.split_text[i:i + n - 1]\n",
    "            context_list.append(context)\n",
    "            ngram_list.append(ngram)\n",
    "        for i in self.split_text:\n",
    "             vocab.update([i])\n",
    "        vocab = list(vocab)\n",
    "            \n",
    "        return ngram_list, context_list, vocab\n",
    "\n",
    "    def counts(self, ingram):\n",
    "        ngramcount = 0\n",
    "        contextcount = 0\n",
    "        ngram_list = self.ngram_context()[0]\n",
    "        context_list = self.ngram_context()[1]\n",
    "\n",
    "        for i in ngram_list:\n",
    "            if i == ingram:\n",
    "                ngramcount += 1\n",
    "\n",
    "        for i in context_list:\n",
    "            if i == ingram[:-1]:\n",
    "                contextcount += 1\n",
    "\n",
    "        return ngramcount, contextcount\n",
    "\n",
    "    def probability(self, ingram): #calculates the normal proability of the ngram\n",
    "        ngram_list, context_list,vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "\n",
    "        # Check for division by zero\n",
    "        if ccount == 0:\n",
    "            return 0.0\n",
    "\n",
    "        prob = ncount / ccount\n",
    "        return prob\n",
    "    \n",
    "    def probability_laplace(self, ingram): #calculates the laplace proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        lprob = (ncount +1)/ (ccount + len(vocab))\n",
    "        return lprob\n",
    "    \n",
    "    def probability_addk(self, ingram, k): #calculates the add-k proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        kprob = (ncount + k)/ (ccount + k*len(vocab))\n",
    "        return kprob\n",
    "        \n",
    "\n",
    "    def perplexity(self): #calculates perplexity using normal probability\n",
    "        ngram_list = self.ngram_context()\n",
    "        prob = 1\n",
    "        for i in ngram_list:\n",
    "            prob_i = self.probability(i)\n",
    "\n",
    "            # Check if prob_i is 0.0, and if so, assign a small positive value (e.g., 1e-10)\n",
    "            if prob_i == 0.0:\n",
    "                prob_i = 1e-10\n",
    "\n",
    "            prob = prob * prob_i\n",
    "\n",
    "        # Check if prob is still 0.0 after the loop\n",
    "        if prob == 0.0:\n",
    "            return float('inf')  # Return infinity to indicate undefined perplexity\n",
    "        else:\n",
    "            perp = prob ** (-1 / self.n)\n",
    "            return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['is', 'a'], ['a', 'test'], ['test', 'it'], ['it', 'has'], ['has', 'some'], ['some', 'punctuation'], ['punctuation', 'this'], ['this', 'like'], ['like', 'commas'], ['commas', 'and'], ['and', 'this'], ['this', 'periods']]\n",
      "[['this'], ['is'], ['a'], ['test'], ['it'], ['has'], ['some'], ['punctuation'], ['this'], ['like'], ['commas'], ['and'], ['this']]\n",
      "['some', 'like', 'commas', 'a', 'it', 'periods', 'is', 'test', 'has', 'punctuation', 'and', 'this']\n",
      "1\n",
      "3\n",
      "0.3333333333333333\n",
      "0.13333333333333333\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test. It has some punctuation, this like commas and  this periods.\"\n",
    "ngram = Ngram(text, n=2)\n",
    "ngram_list, context_list, vocab = ngram.ngram_context()\n",
    "print(ngram_list)\n",
    "print(context_list)\n",
    "print(vocab)\n",
    "ngramcount  = ngram.counts([ 'this', 'is'])[0]\n",
    "contextcount = ngram.counts([ 'this', 'is'])[1]\n",
    "print(ngramcount) \n",
    "print(contextcount)\n",
    "prob = ngram.probability(['this', 'is'])\n",
    "lprob = ngram.probability_laplace(['this', 'is'])\n",
    "kprob = ngram.probability_addk(['this', 'is'], 0.5)\n",
    "print(prob)\n",
    "print(lprob)\n",
    "print(kprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes (with laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very Powerful', 'the most fun film of the summer', 'no surprises and very few laughs', 'entirely predictable and lacks energy', 'just plain boring']\n",
      "Prior positive: 0.4\n",
      "Prior negative: 0.6\n",
      "['summer', 'very', 'laughs', 'most', 'the', 'plain', 'few', 'powerful', 'lacks', 'boring', 'and', 'no', 'energy', 'predictable', 'film', 'surprises', 'entirely', 'fun', 'just', 'of']\n",
      "positive_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "negative_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "The positve probability is 7.513148009015778e-05\n",
      "The negative probability is 0.00019725486972959643\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"Very Powerful\", \"the most fun film of the summer\", \"no surprises and very few laughs\", \"entirely predictable and lacks energy\",\"just plain boring\"]\n",
    "print(texts)\n",
    "labels = [1, 1, 0, 0,0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Calculate the prior probabilities\n",
    "total_samples = len(texts)\n",
    "positive_samples = sum(labels)\n",
    "negative_samples = total_samples - positive_samples\n",
    "prior_positive = positive_samples / total_samples\n",
    "prior_negative = negative_samples / total_samples\n",
    "print(f\"Prior positive: {prior_positive}\")\n",
    "print(f\"Prior negative: {prior_negative}\")\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "# Calculate word frequencies in positive and negative classes\n",
    "positive_word_counts = {word: 0 for word in vocabulary}\n",
    "negative_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"positive_word_counts is {positive_word_counts}\")\n",
    "print(f\"negative_word_counts is {negative_word_counts}\")\n",
    "\n",
    "for i, text in enumerate(texts): #enumerate generates index and value\n",
    "    words = preprocess_text(text)\n",
    "    for word in words:\n",
    "        if labels[i] == 1:\n",
    "            positive_word_counts[word] += 1\n",
    "        else:\n",
    "            negative_word_counts[word] += 1\n",
    "\n",
    "# Calculate conditional probabilities (likelihoods)\n",
    "smooth_factor = 1  # Laplace smoothing to avoid zero probabilities\n",
    "positive_likelihoods = {}\n",
    "negative_likelihoods = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    positive_likelihoods[word] = (positive_word_counts[word] + smooth_factor) / (positive_samples + smooth_factor * len(vocabulary))\n",
    "    negative_likelihoods[word] = (negative_word_counts[word] + smooth_factor) / (negative_samples + smooth_factor * len(vocabulary))\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    prob_positive = prior_positive\n",
    "    prob_negative = prior_negative\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            prob_positive *= positive_likelihoods[word]\n",
    "            prob_negative *= negative_likelihoods[word]\n",
    "    \n",
    "    if prob_positive > prob_negative:\n",
    "        return 1,prob_negative,prob_positive  # Positive class\n",
    "    else:\n",
    "        return 0,prob_negative,prob_positive  # Negative class\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"predictable with no fun\"\n",
    "predicted_label,prob_negative,prob_positive = classify_text(new_text)\n",
    "print(f\"The positve probability is {prob_positive}\")\n",
    "print(f\"The negative probability is {prob_negative}\")\n",
    "if predicted_label == 1:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (sigmoid activation function, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'a', 'sentiment', 'example', 'positive', 'negative', 'here', 'another', 'is', 'sentence', 'more', 'this']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[1 0 1 0]\n",
      "[[0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary))) #text is number of rows and vocabulary is number of columns\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts): #gives index and value\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary) #we create number of features equal to length of vocabulary for simplicity\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01 #The learning rate determines how much weights and bias are updated during each iteration of gradient descent.\n",
    "num_epochs = 1000 #The number of epochs determines how many times the model will loop through the entire training dataset.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    predictions = sigmoid(np.dot(X, weights) + bias) #sigma(w.x+b)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = sigmoid(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (softmax activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary)))\n",
    "y = np.array(labels)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts):\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary)\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z) \n",
    "    return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # Small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to prevent log(0)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, weights) + bias\n",
    "    predictions = softmax(z)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = softmax(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "the tf-idf\n",
    " model, an important baseline, the meaning of a word is defifined by a simple function\n",
    " of the counts of nearby words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = [\n",
    "    \"Natural language processing has its roots in the 1950s.\",\n",
    "    \"Already in 1950, Alan Turing published an article titled 'Computing Machinery and Intelligence'\",\n",
    "    \"It proposed what is now called the Turing test as a criterion of intelligence\",\n",
    "    \"though at the time that was not articulated as a problem separate from artificial intelligence.\"\n",
    "    \"The proposed test includes a task that involves the automated interpretation and generation of natural language.\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(corpus):\n",
    "    vocab = []\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.split(' '):\n",
    "            if len(word)>1 and word not in vocab:\n",
    "                vocab.append(word)\n",
    "\n",
    "    word_dimension = {j:i for i,j in enumerate(vocab)}\n",
    "    return word_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimension = vocab(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Natural': 0,\n",
       " 'language': 1,\n",
       " 'processing': 2,\n",
       " 'has': 3,\n",
       " 'its': 4,\n",
       " 'roots': 5,\n",
       " 'in': 6,\n",
       " 'the': 7,\n",
       " '1950s.': 8,\n",
       " 'Already': 9,\n",
       " '1950,': 10,\n",
       " 'Alan': 11,\n",
       " 'Turing': 12,\n",
       " 'published': 13,\n",
       " 'an': 14,\n",
       " 'article': 15,\n",
       " 'titled': 16,\n",
       " \"'Computing\": 17,\n",
       " 'Machinery': 18,\n",
       " 'and': 19,\n",
       " \"Intelligence'\": 20,\n",
       " 'It': 21,\n",
       " 'proposed': 22,\n",
       " 'what': 23,\n",
       " 'is': 24,\n",
       " 'now': 25,\n",
       " 'called': 26,\n",
       " 'test': 27,\n",
       " 'as': 28,\n",
       " 'criterion': 29,\n",
       " 'of': 30,\n",
       " 'intelligence': 31,\n",
       " 'though': 32,\n",
       " 'at': 33,\n",
       " 'time': 34,\n",
       " 'that': 35,\n",
       " 'was': 36,\n",
       " 'not': 37,\n",
       " 'articulated': 38,\n",
       " 'problem': 39,\n",
       " 'separate': 40,\n",
       " 'from': 41,\n",
       " 'artificial': 42,\n",
       " 'intelligence.The': 43,\n",
       " 'includes': 44,\n",
       " 'task': 45,\n",
       " 'involves': 46,\n",
       " 'automated': 47,\n",
       " 'interpretation': 48,\n",
       " 'generation': 49,\n",
       " 'natural': 50,\n",
       " 'language.': 51}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(corpus, word):\n",
    "    count = 0\n",
    "    for sentence in corpus:\n",
    "        if word in sentence:\n",
    "            count = count+1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "def transform(corpus, word_dimension):\n",
    "    # Initialize lists to store document indices, column indices, and TF-IDF values\n",
    "    documents = []\n",
    "    columns = []\n",
    "    tf_idf_values = []\n",
    "\n",
    "    # Iterate over each document in the corpus\n",
    "    for index, document in enumerate(corpus):\n",
    "        # Calculate word frequencies in the current document\n",
    "        word_frequency = Counter(document.split())\n",
    "\n",
    "        # Iterate over unique words and their frequencies in the document\n",
    "        for word, freq in word_frequency.items():\n",
    "            # Skip short words\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "\n",
    "            # Get the index of the word from word_dimension, default to -1 if not found\n",
    "            dimension_index = word_dimension.get(word, -1) #how many times the word is in a document\n",
    "\n",
    "            # If the word is in word_dimension, calculate its TF-IDF and store information\n",
    "            if dimension_index != -1:\n",
    "                documents.append(index)  # Document index\n",
    "                columns.append(dimension_index)  # Word index in vocabulary\n",
    "\n",
    "                # Calculate TF-IDF using the formula\n",
    "                tf_idf_value = (freq / len(document.split())) * (1 + (np.log((1 + len(corpus)) / (1 + word_dimension[word]))))\n",
    "\n",
    "                tf_idf_values.append(tf_idf_value)\n",
    "\n",
    "    # Create a sparse matrix using CSR format\n",
    "    # number of rows is equal to the number of documents in the corpus, and the number of columns is equal to the number of unique words in the vocabulary defined by the word_dimension dictionary.\n",
    "    sparse_matrix = csr_matrix((tf_idf_values, (documents, columns)), shape=(len(corpus), len(word_dimension)))\n",
    "\n",
    "    # Normalize the TF-IDF matrix\n",
    "    final_normalized = normalize(sparse_matrix)\n",
    "    print(word_frequency)\n",
    "    print(documents)\n",
    "    print(columns)\n",
    "\n",
    "    return final_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 2, 'that': 2, 'a': 2, 'though': 1, 'at': 1, 'time': 1, 'was': 1, 'not': 1, 'articulated': 1, 'as': 1, 'problem': 1, 'separate': 1, 'from': 1, 'artificial': 1, 'intelligence.The': 1, 'proposed': 1, 'test': 1, 'includes': 1, 'task': 1, 'involves': 1, 'automated': 1, 'interpretation': 1, 'and': 1, 'generation': 1, 'of': 1, 'natural': 1, 'language.': 1})\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 7, 12, 27, 28, 29, 30, 31, 32, 33, 7, 34, 35, 36, 37, 38, 28, 39, 40, 41, 42, 43, 22, 27, 44, 45, 46, 47, 48, 19, 49, 30, 50, 51]\n"
     ]
    }
   ],
   "source": [
    "vals = transform(text_1, word_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.6362648423548146\n",
      "  (0, 1)\t0.4672532788046143\n",
      "  (0, 2)\t0.3683878519394966\n",
      "  (0, 3)\t0.29824171525441406\n",
      "  (0, 4)\t0.24383214458676375\n",
      "  (0, 5)\t0.19937628838929639\n",
      "  (0, 6)\t0.16178939753750837\n",
      "  (0, 7)\t0.12923015170421384\n",
      "  (0, 8)\t0.10051086152417864\n",
      "  (1, 6)\t0.6023037709341442\n",
      "  (1, 9)\t0.27853937765828185\n",
      "  (1, 10)\t0.1920235091854428\n",
      "  (1, 11)\t0.1130407094180897\n",
      "  (1, 12)\t0.040383576848297284\n",
      "  (1, 13)\t-0.026886420803614346\n",
      "  (1, 14)\t-0.08951329032986134\n",
      "  (1, 15)\t-0.14809681433152597\n",
      "  (1, 16)\t-0.20312757618775612\n",
      "  (1, 17)\t-0.2550119585700533\n",
      "  (1, 18)\t-0.3040903740521614\n",
      "  (1, 19)\t-0.35065081407947685\n",
      "  (1, 20)\t-0.39493908879175754\n",
      "  (2, 7)\t0.22568480161358573\n",
      "  (2, 12)\t0.018944263125366588\n",
      "  (2, 21)\t-0.20507843312209903\n",
      "  :\t:\n",
      "  (3, 19)\t-0.06815960845379375\n",
      "  (3, 22)\t-0.09281986816200104\n",
      "  (3, 27)\t-0.12752836505999324\n",
      "  (3, 28)\t-0.13372004389146525\n",
      "  (3, 30)\t-0.14548738618426157\n",
      "  (3, 32)\t-0.1565187743719772\n",
      "  (3, 33)\t-0.16178617271962303\n",
      "  (3, 34)\t-0.16690087115216784\n",
      "  (3, 35)\t-0.34374294847821546\n",
      "  (3, 36)\t-0.176705879140291\n",
      "  (3, 37)\t-0.18141135108660564\n",
      "  (3, 38)\t-0.18599458905443506\n",
      "  (3, 39)\t-0.19046178312404236\n",
      "  (3, 40)\t-0.19481866475436624\n",
      "  (3, 41)\t-0.1990705509987375\n",
      "  (3, 42)\t-0.20322238351721442\n",
      "  (3, 43)\t-0.20727876310348162\n",
      "  (3, 44)\t-0.21124398033128233\n",
      "  (3, 45)\t-0.21512204283224962\n",
      "  (3, 46)\t-0.21891669963991886\n",
      "  (3, 47)\t-0.22263146297061212\n",
      "  (3, 48)\t-0.22626962775836731\n",
      "  (3, 49)\t-0.22983428921621687\n",
      "  (3, 50)\t-0.2333283586583673\n",
      "  (3, 51)\t-0.23675457778593936\n"
     ]
    }
   ],
   "source": [
    "print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (using scikit-learn, needs TF-IDF vecotrization definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'imdb_reviews.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t', quotechar=\"'\", escapechar='\\\\', header=None, names=['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[:500]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab1(corpus):\n",
    "    vocab = []\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.split(' '):\n",
    "            if len(word)>1 and word not in vocab:\n",
    "                vocab.append(word)\n",
    "\n",
    "    word_dimension = {j:i for i,j in enumerate(vocab)}\n",
    "    return word_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m word_dimesn \u001b[39m=\u001b[39m vocab1(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_ \u001b[39m=\u001b[39m transform(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m], word_dimesn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df1['review'])\n",
    "word_dimesn = vocab1(df1['review'])\n",
    "X_ = transform(df1['review'], word_dimesn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
