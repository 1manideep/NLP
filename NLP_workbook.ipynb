{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Models (without using any libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, text, n=None):\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.split_text = self.process()\n",
    "\n",
    "    def process(self):\n",
    "        text = self.text.lower()\n",
    "        text = text.replace('.', ' ')\n",
    "        text = text.replace(',', ' ')\n",
    "        split_text = text.split()\n",
    "        return split_text\n",
    "\n",
    "    def ngram_context(self):\n",
    "        ngram_list, context_list  = [], []\n",
    "        vocab = set()\n",
    "        n = self.n\n",
    "        for i in range(0, len(self.split_text) - n + 1):\n",
    "            ngram = self.split_text[i:i + n]\n",
    "            context = self.split_text[i:i + n - 1]\n",
    "            context_list.append(context)\n",
    "            ngram_list.append(ngram)\n",
    "        for i in self.split_text:\n",
    "             vocab.update([i])\n",
    "        vocab = list(vocab)\n",
    "            \n",
    "        return ngram_list, context_list, vocab\n",
    "\n",
    "    def counts(self, ingram):\n",
    "        ngramcount = 0\n",
    "        contextcount = 0\n",
    "        ngram_list = self.ngram_context()[0]\n",
    "        context_list = self.ngram_context()[1]\n",
    "\n",
    "        for i in ngram_list:\n",
    "            if i == ingram:\n",
    "                ngramcount += 1\n",
    "\n",
    "        for i in context_list:\n",
    "            if i == ingram[:-1]:\n",
    "                contextcount += 1\n",
    "\n",
    "        return ngramcount, contextcount\n",
    "\n",
    "    def probability(self, ingram): #calculates the normal proability of the ngram\n",
    "        ngram_list, context_list,vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "\n",
    "        # Check for division by zero\n",
    "        if ccount == 0:\n",
    "            return 0.0\n",
    "\n",
    "        prob = ncount / ccount\n",
    "        return prob\n",
    "    \n",
    "    def probability_laplace(self, ingram): #calculates the laplace proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        lprob = (ncount +1)/ (ccount + len(vocab))\n",
    "        return lprob\n",
    "    \n",
    "    def probability_addk(self, ingram, k): #calculates the add-k proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        kprob = (ncount + k)/ (ccount + k*len(vocab))\n",
    "        return kprob\n",
    "        \n",
    "\n",
    "    def perplexity(self): #calculates perplexity using normal probability\n",
    "        ngram_list = self.ngram_context()\n",
    "        prob = 1\n",
    "        for i in ngram_list:\n",
    "            prob_i = self.probability(i)\n",
    "\n",
    "            # Check if prob_i is 0.0, and if so, assign a small positive value (e.g., 1e-10)\n",
    "            if prob_i == 0.0:\n",
    "                prob_i = 1e-10\n",
    "\n",
    "            prob = prob * prob_i\n",
    "\n",
    "        # Check if prob is still 0.0 after the loop\n",
    "        if prob == 0.0:\n",
    "            return float('inf')  # Return infinity to indicate undefined perplexity\n",
    "        else:\n",
    "            perp = prob ** (-1 / self.n)\n",
    "            return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['is', 'a'], ['a', 'test'], ['test', 'it'], ['it', 'has'], ['has', 'some'], ['some', 'punctuation'], ['punctuation', 'this'], ['this', 'like'], ['like', 'commas'], ['commas', 'and'], ['and', 'this'], ['this', 'periods']]\n",
      "[['this'], ['is'], ['a'], ['test'], ['it'], ['has'], ['some'], ['punctuation'], ['this'], ['like'], ['commas'], ['and'], ['this']]\n",
      "['some', 'like', 'commas', 'a', 'it', 'periods', 'is', 'test', 'has', 'punctuation', 'and', 'this']\n",
      "1\n",
      "3\n",
      "0.3333333333333333\n",
      "0.13333333333333333\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test. It has some punctuation, this like commas and  this periods.\"\n",
    "ngram = Ngram(text, n=2)\n",
    "ngram_list, context_list, vocab = ngram.ngram_context()\n",
    "print(ngram_list)\n",
    "print(context_list)\n",
    "print(vocab)\n",
    "ngramcount  = ngram.counts([ 'this', 'is'])[0]\n",
    "contextcount = ngram.counts([ 'this', 'is'])[1]\n",
    "print(ngramcount) \n",
    "print(contextcount)\n",
    "prob = ngram.probability(['this', 'is'])\n",
    "lprob = ngram.probability_laplace(['this', 'is'])\n",
    "kprob = ngram.probability_addk(['this', 'is'], 0.5)\n",
    "print(prob)\n",
    "print(lprob)\n",
    "print(kprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes (with laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very Powerful', 'the most fun film of the summer', 'no surprises and very few laughs', 'entirely predictable and lacks energy', 'just plain boring']\n",
      "Prior positive: 0.4\n",
      "Prior negative: 0.6\n",
      "['summer', 'very', 'laughs', 'most', 'the', 'plain', 'few', 'powerful', 'lacks', 'boring', 'and', 'no', 'energy', 'predictable', 'film', 'surprises', 'entirely', 'fun', 'just', 'of']\n",
      "positive_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "negative_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "The positve probability is 7.513148009015778e-05\n",
      "The negative probability is 0.00019725486972959643\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"Very Powerful\", \"the most fun film of the summer\", \"no surprises and very few laughs\", \"entirely predictable and lacks energy\",\"just plain boring\"]\n",
    "print(texts)\n",
    "labels = [1, 1, 0, 0,0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Calculate the prior probabilities\n",
    "total_samples = len(texts)\n",
    "positive_samples = sum(labels)\n",
    "negative_samples = total_samples - positive_samples\n",
    "prior_positive = positive_samples / total_samples\n",
    "prior_negative = negative_samples / total_samples\n",
    "print(f\"Prior positive: {prior_positive}\")\n",
    "print(f\"Prior negative: {prior_negative}\")\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "# Calculate word frequencies in positive and negative classes\n",
    "positive_word_counts = {word: 0 for word in vocabulary}\n",
    "negative_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"positive_word_counts is {positive_word_counts}\")\n",
    "print(f\"negative_word_counts is {negative_word_counts}\")\n",
    "\n",
    "for i, text in enumerate(texts): #enumerate generates index and value\n",
    "    words = preprocess_text(text)\n",
    "    for word in words:\n",
    "        if labels[i] == 1:\n",
    "            positive_word_counts[word] += 1\n",
    "        else:\n",
    "            negative_word_counts[word] += 1\n",
    "\n",
    "# Calculate conditional probabilities (likelihoods)\n",
    "smooth_factor = 1  # Laplace smoothing to avoid zero probabilities\n",
    "positive_likelihoods = {}\n",
    "negative_likelihoods = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    positive_likelihoods[word] = (positive_word_counts[word] + smooth_factor) / (positive_samples + smooth_factor * len(vocabulary))\n",
    "    negative_likelihoods[word] = (negative_word_counts[word] + smooth_factor) / (negative_samples + smooth_factor * len(vocabulary))\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    prob_positive = prior_positive\n",
    "    prob_negative = prior_negative\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            prob_positive *= positive_likelihoods[word]\n",
    "            prob_negative *= negative_likelihoods[word]\n",
    "    \n",
    "    if prob_positive > prob_negative:\n",
    "        return 1,prob_negative,prob_positive  # Positive class\n",
    "    else:\n",
    "        return 0,prob_negative,prob_positive  # Negative class\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"predictable with no fun\"\n",
    "predicted_label,prob_negative,prob_positive = classify_text(new_text)\n",
    "print(f\"The positve probability is {prob_positive}\")\n",
    "print(f\"The negative probability is {prob_negative}\")\n",
    "if predicted_label == 1:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (sigmoid activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary)))\n",
    "y = np.array(labels)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts):\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary)\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    predictions = sigmoid(np.dot(X, weights) + bias)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = sigmoid(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (softmax activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary)))\n",
    "y = np.array(labels)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts):\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary)\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))  # Subtract max(z) to prevent overflow\n",
    "    return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # Small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to prevent log(0)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, weights) + bias\n",
    "    predictions = softmax(z)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = softmax(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (using scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://bookdown.org/f_lennert/book-toolbox_css/text-mining.html#tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 5 fields in line 18, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mimdb_reviews.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    616\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1741\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1742\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1743\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     (\n\u001b[1;32m   1745\u001b[0m         index,\n\u001b[1;32m   1746\u001b[0m         columns,\n\u001b[1;32m   1747\u001b[0m         col_dict,\n\u001b[0;32m-> 1748\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1749\u001b[0m         nrows\n\u001b[1;32m   1750\u001b[0m     )\n\u001b[1;32m   1751\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:904\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5 fields in line 18, saw 7\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
