{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Models (without using any libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, text, n=None):\n",
    "        self.text = text\n",
    "        self.n = n\n",
    "        self.split_text = self.process()\n",
    "\n",
    "    def process(self):\n",
    "        text = self.text.lower()\n",
    "        text = text.replace('.', ' ')\n",
    "        text = text.replace(',', ' ')\n",
    "        split_text = text.split()\n",
    "        return split_text\n",
    "\n",
    "    def ngram_context(self):\n",
    "        ngram_list, context_list  = [], []\n",
    "        vocab = set()\n",
    "        n = self.n\n",
    "        for i in range(0, len(self.split_text) - n + 1):\n",
    "            ngram = self.split_text[i:i + n]\n",
    "            context = self.split_text[i:i + n - 1]\n",
    "            context_list.append(context)\n",
    "            ngram_list.append(ngram)\n",
    "        for i in self.split_text:\n",
    "             vocab.update([i])\n",
    "        vocab = list(vocab)\n",
    "            \n",
    "        return ngram_list, context_list, vocab\n",
    "\n",
    "    def counts(self, ingram):\n",
    "        ngramcount = 0\n",
    "        contextcount = 0\n",
    "        ngram_list = self.ngram_context()[0]\n",
    "        context_list = self.ngram_context()[1]\n",
    "\n",
    "        for i in ngram_list:\n",
    "            if i == ingram:\n",
    "                ngramcount += 1\n",
    "\n",
    "        for i in context_list:\n",
    "            if i == ingram[:-1]:\n",
    "                contextcount += 1\n",
    "\n",
    "        return ngramcount, contextcount\n",
    "\n",
    "    def probability(self, ingram): #calculates the normal proability of the ngram\n",
    "        ngram_list, context_list,vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "\n",
    "        # Check for division by zero\n",
    "        if ccount == 0:\n",
    "            return 0.0\n",
    "\n",
    "        prob = ncount / ccount\n",
    "        return prob\n",
    "    \n",
    "    def probability_laplace(self, ingram): #calculates the laplace proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        lprob = (ncount +1)/ (ccount + len(vocab))\n",
    "        return lprob\n",
    "    \n",
    "    def probability_addk(self, ingram, k): #calculates the add-k proability of the ngram\n",
    "        ngram_list, context_list, vocab = self.ngram_context()\n",
    "        ncount, ccount = self.counts(ingram)\n",
    "        \n",
    "        kprob = (ncount + k)/ (ccount + k*len(vocab))\n",
    "        return kprob\n",
    "        \n",
    "\n",
    "    def perplexity(self): #calculates perplexity using normal probability\n",
    "        ngram_list = self.ngram_context()\n",
    "        prob = 1\n",
    "        for i in ngram_list:\n",
    "            prob_i = self.probability(i)\n",
    "\n",
    "            # Check if prob_i is 0.0, and if so, assign a small positive value (e.g., 1e-10)\n",
    "            if prob_i == 0.0:\n",
    "                prob_i = 1e-10\n",
    "\n",
    "            prob = prob * prob_i\n",
    "\n",
    "        # Check if prob is still 0.0 after the loop\n",
    "        if prob == 0.0:\n",
    "            return float('inf')  # Return infinity to indicate undefined perplexity\n",
    "        else:\n",
    "            perp = prob ** (-1 / self.n)\n",
    "            return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['is', 'a'], ['a', 'test'], ['test', 'it'], ['it', 'has'], ['has', 'some'], ['some', 'punctuation'], ['punctuation', 'this'], ['this', 'like'], ['like', 'commas'], ['commas', 'and'], ['and', 'this'], ['this', 'periods']]\n",
      "[['this'], ['is'], ['a'], ['test'], ['it'], ['has'], ['some'], ['punctuation'], ['this'], ['like'], ['commas'], ['and'], ['this']]\n",
      "['some', 'like', 'commas', 'a', 'it', 'periods', 'is', 'test', 'has', 'punctuation', 'and', 'this']\n",
      "1\n",
      "3\n",
      "0.3333333333333333\n",
      "0.13333333333333333\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test. It has some punctuation, this like commas and  this periods.\"\n",
    "ngram = Ngram(text, n=2)\n",
    "ngram_list, context_list, vocab = ngram.ngram_context()\n",
    "print(ngram_list)\n",
    "print(context_list)\n",
    "print(vocab)\n",
    "ngramcount  = ngram.counts([ 'this', 'is'])[0]\n",
    "contextcount = ngram.counts([ 'this', 'is'])[1]\n",
    "print(ngramcount) \n",
    "print(contextcount)\n",
    "prob = ngram.probability(['this', 'is'])\n",
    "lprob = ngram.probability_laplace(['this', 'is'])\n",
    "kprob = ngram.probability_addk(['this', 'is'], 0.5)\n",
    "print(prob)\n",
    "print(lprob)\n",
    "print(kprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive-Bayes (with laplace smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Very Powerful', 'the most fun film of the summer', 'no surprises and very few laughs', 'entirely predictable and lacks energy', 'just plain boring']\n",
      "Prior positive: 0.4\n",
      "Prior negative: 0.6\n",
      "['summer', 'very', 'laughs', 'most', 'the', 'plain', 'few', 'powerful', 'lacks', 'boring', 'and', 'no', 'energy', 'predictable', 'film', 'surprises', 'entirely', 'fun', 'just', 'of']\n",
      "positive_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "negative_word_counts is {'summer': 0, 'very': 0, 'laughs': 0, 'most': 0, 'the': 0, 'plain': 0, 'few': 0, 'powerful': 0, 'lacks': 0, 'boring': 0, 'and': 0, 'no': 0, 'energy': 0, 'predictable': 0, 'film': 0, 'surprises': 0, 'entirely': 0, 'fun': 0, 'just': 0, 'of': 0}\n",
      "The positve probability is 7.513148009015778e-05\n",
      "The negative probability is 0.00019725486972959643\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"Very Powerful\", \"the most fun film of the summer\", \"no surprises and very few laughs\", \"entirely predictable and lacks energy\",\"just plain boring\"]\n",
    "print(texts)\n",
    "labels = [1, 1, 0, 0,0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Calculate the prior probabilities\n",
    "total_samples = len(texts)\n",
    "positive_samples = sum(labels)\n",
    "negative_samples = total_samples - positive_samples\n",
    "prior_positive = positive_samples / total_samples\n",
    "prior_negative = negative_samples / total_samples\n",
    "print(f\"Prior positive: {prior_positive}\")\n",
    "print(f\"Prior negative: {prior_negative}\")\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "# Calculate word frequencies in positive and negative classes\n",
    "positive_word_counts = {word: 0 for word in vocabulary}\n",
    "negative_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"positive_word_counts is {positive_word_counts}\")\n",
    "print(f\"negative_word_counts is {negative_word_counts}\")\n",
    "\n",
    "for i, text in enumerate(texts): #enumerate generates index and value\n",
    "    words = preprocess_text(text)\n",
    "    for word in words:\n",
    "        if labels[i] == 1:\n",
    "            positive_word_counts[word] += 1\n",
    "        else:\n",
    "            negative_word_counts[word] += 1\n",
    "\n",
    "# Calculate conditional probabilities (likelihoods)\n",
    "smooth_factor = 1  # Laplace smoothing to avoid zero probabilities\n",
    "positive_likelihoods = {}\n",
    "negative_likelihoods = {}\n",
    "\n",
    "for word in vocabulary:\n",
    "    positive_likelihoods[word] = (positive_word_counts[word] + smooth_factor) / (positive_samples + smooth_factor * len(vocabulary))\n",
    "    negative_likelihoods[word] = (negative_word_counts[word] + smooth_factor) / (negative_samples + smooth_factor * len(vocabulary))\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    prob_positive = prior_positive\n",
    "    prob_negative = prior_negative\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            prob_positive *= positive_likelihoods[word]\n",
    "            prob_negative *= negative_likelihoods[word]\n",
    "    \n",
    "    if prob_positive > prob_negative:\n",
    "        return 1,prob_negative,prob_positive  # Positive class\n",
    "    else:\n",
    "        return 0,prob_negative,prob_positive  # Negative class\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"predictable with no fun\"\n",
    "predicted_label,prob_negative,prob_positive = classify_text(new_text)\n",
    "print(f\"The positve probability is {prob_positive}\")\n",
    "print(f\"The negative probability is {prob_negative}\")\n",
    "if predicted_label == 1:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (sigmoid activation function, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'a', 'sentiment', 'example', 'positive', 'negative', 'here', 'another', 'is', 'sentence', 'more', 'this']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[1 0 1 0]\n",
      "[[0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "print(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary))) #text is number of rows and vocabulary is number of columns\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts): #gives index and value\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary) #we create number of features equal to length of vocabulary for simplicity\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01 #The learning rate determines how much weights and bias are updated during each iteration of gradient descent.\n",
    "num_epochs = 1000 #The number of epochs determines how many times the model will loop through the entire training dataset.\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    predictions = sigmoid(np.dot(X, weights) + bias) #sigma(w.x+b)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = sigmoid(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (softmax activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample text data (replace with your own dataset)\n",
    "texts = [\"This is a positive sentence.\", \"Negative sentiment here.\", \"Another positive example.\", \"More negative text.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.replace('.', '')  # Remove periods\n",
    "    text = text.replace(',', '')  # Remove commas\n",
    "    text = text.replace('!', '')  # Remove exclamation marks\n",
    "    text = text.replace('?', '')  # Remove question marks\n",
    "    text = text.replace('\"', '')  # Remove double quotes\n",
    "    text = text.replace(\"'\", '')  # Remove single quotes\n",
    "    text = text.replace('(', '')  # Remove opening parentheses\n",
    "    text = text.replace(')', '')  # Remove closing parentheses\n",
    "    text = text.split()  # Tokenize by whitespace\n",
    "    return text\n",
    "\n",
    "# Create a vocabulary of unique words\n",
    "vocabulary = set()\n",
    "for text in texts:\n",
    "    words = preprocess_text(text)\n",
    "    vocabulary.update(words)\n",
    "vocabulary = list(vocabulary)\n",
    "\n",
    "# Create a feature matrix (X) and target vector (y)\n",
    "X = np.zeros((len(texts), len(vocabulary)))\n",
    "y = np.array(labels)\n",
    "\n",
    "# Convert text data into a binary bag-of-words representation\n",
    "for i, text in enumerate(texts):\n",
    "    words = preprocess_text(text)\n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            X[i][j] = 1\n",
    "\n",
    "# Initialize weights and bias\n",
    "num_features = len(vocabulary)\n",
    "weights = np.zeros(num_features)\n",
    "bias = 0\n",
    "\n",
    "# Softmax activation function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z) \n",
    "    return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "# Define the cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # Small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to prevent log(0)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# Train the logistic regression model using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, weights) + bias\n",
    "    predictions = softmax(z)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1 / len(texts)) * np.dot(X.T, (predictions - y))\n",
    "    db = (1 / len(texts)) * np.sum(predictions - y)\n",
    "    \n",
    "    # Update weights and bias\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db\n",
    "\n",
    "# Classify new text\n",
    "def classify_text(text):\n",
    "    words = preprocess_text(text)\n",
    "    input_features = np.zeros(num_features)\n",
    "    \n",
    "    for j, word in enumerate(vocabulary):\n",
    "        if word in words:\n",
    "            input_features[j] = 1\n",
    "    \n",
    "    prediction = softmax(np.dot(input_features, weights) + bias)\n",
    "    return prediction\n",
    "\n",
    "# Test the classifier\n",
    "new_text = \"This is a test of the Logistic Regression classifier.\"\n",
    "predicted_prob = classify_text(new_text)\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization\n",
    "the tf-idf\n",
    " model, an important baseline, the meaning of a word is defifined by a simple function\n",
    " of the counts of nearby words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (using scikit-learn, needs TF-IDF vecotrization definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'imdb_reviews.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8', delimiter='\\t', quotechar=\"'\", escapechar='\\\\', header=None, names=['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3       Very little music or anything to speak of.        0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df[:500]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab1(corpus):\n",
    "    vocab = []\n",
    "    for sentence in corpus:\n",
    "        for word in sentence.split(' '):\n",
    "            if len(word)>1 and word not in vocab:\n",
    "                vocab.append(word)\n",
    "\n",
    "    word_dimension = {j:i for i,j in enumerate(vocab)}\n",
    "    return word_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m word_dimesn \u001b[39m=\u001b[39m vocab1(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/manideep/Downloads/NLP/git_repo/NLP/NLP_workbook.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_ \u001b[39m=\u001b[39m transform(df1[\u001b[39m'\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m'\u001b[39m], word_dimesn)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df1['review'])\n",
    "word_dimesn = vocab1(df1['review'])\n",
    "X_ = transform(df1['review'], word_dimesn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
